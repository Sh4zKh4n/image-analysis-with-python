{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (BioImage) Data Analysis with Python\n",
    "\n",
    "*created June 2018 by Jonas Hartmann (Gilmour group, EMBL Heidelberg)*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [About this Tutorial](#about)\n",
    "2. [Preparations](#prep)\n",
    "    1. [Imports](#imports)\n",
    "    2. [Loading the Data](#loading)\n",
    "    3. [Some Checks for Common Problems](#checks)\n",
    "3. [Basic Data Visualization](#dataviz)\n",
    "    1. [Basic Boxplot](#bplot)\n",
    "    2. [Interactive Scatterplot](#iscatter)\n",
    "    3. [Interactive Backmapping](#ibackmap)\n",
    "4. [Multi-Dimensional Analysis](#MDA)\n",
    "    1. [Feature Standardization](#featstand)\n",
    "    2. [Dimensionality Reduction by PCA](#pca)\n",
    "    3. [Dimensionality Reduction by tSNE](#tsne)\n",
    "    4. [Clustering with k-Means](#cluster)\n",
    "    5. [Cluster Visualization by Minimum Spanning Tree](#mst)\n",
    "    6. [Classification of Mitotic Cells](#mitotic)\n",
    "    7. [Grouped Analysis and Hypothesis Testing](#grouped_and_hypot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About this Tutorial <a id=about></a>\n",
    "\n",
    "*Analyzing biological image data commonly involves the detection and segmentation of objects of interest such as cells or nuclei, the properties of which can then be measured individually, producing *single-cell data*. However, extracting biological meaning from such data is often far from trivial! Fortunately, a large host of data analysis algorithms and data visualization tools is freely available in the python ecosystem. This tutorial provides an introductory overview of some of the most important tools in the field.*\n",
    "\n",
    "\n",
    "#### <font color=orangered>Warning:</font> This Tutorial is in Beta!\n",
    "\n",
    "It has not been extensively tested yet and may contain flaws both at the conceptional and the implementation level. Furthermore, it has not yet been extended to be not fully self-explanatory!\n",
    "\n",
    "\n",
    "#### Background\n",
    "\n",
    "The images used for this tutorial were downloaded from the [Broad Bioimage Benchmark Collection (BBBC)](https://data.broadinstitute.org/bbbc/index.html), which is a collection of freely downloadable microscopy image sets.\n",
    "\n",
    "They are 3-color images of cultured **HT29 cells**, a widely used human colon cancer cell line. The data was originally produced by *Moffat et al.* in the context of a high-content RNAi screen. The three channels are `Hoechst 33342` (channel named `DNA`, showing the nuclei), `phospho-histone H3` (channel named `pH3`, indicates cells in mitosis), and `phalloidin` (channel named `actin`, shows the actin cytoskeleton). This dataset makes for a very nice example case because the cells are morphologically highly diverse and the pH3 staining allows the classification and analysis of a functionally relevant subset of cells.\n",
    "\n",
    "The images were obtained from [BBBC018](https://data.broadinstitute.org/bbbc/BBBC018/) as `16bit` images in the `.DIB` format and converted into `8bit .tif` images using a simple Fiji macro. Next, nuclei were segmented based on the `DNA` channel and segmentations were extended to capture cell outlines using the `actin` channel (see `\\data\\image_analysis_pipeline_DEV.ipynb` and `\\data\\image_analysis_pipeline_RUN.ipynb`). \n",
    "\n",
    "Features quantifying cell shape and intensity of each channel were extracted using `skimage.measure.regionprops` and converted to a pandas DataFrame, which was then saved in `\\data\\BBBC018_v1_features.pkl`. This file is the starting point for this tutorial.\n",
    "\n",
    "\n",
    "#### Required Modules\n",
    "\n",
    "- Make sure the following modules are installed before you get started:\n",
    "    - numpy\n",
    "    - scipy\n",
    "    - matplotlib\n",
    "    - pandas\n",
    "    - scikit-learn\n",
    "    - networkx\n",
    "    - scikit-image or tifffile (only used for imread function)\n",
    "- All required modules (except tifffile) come pre-installed if you are using the **[Anaconda distribution](https://www.anaconda.com/download/)** of python. \n",
    "- To install tifffile, use `conda install -c conda-forge tifffile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparations <a id=prep></a>\n",
    "\n",
    "In this section we import the required modules, load the data and prepare it for analysis.\n",
    "\n",
    "Importantly, we check the data for some of the most common problems/mistakes that can sneak into such datasets. Although this step seems trivial, it is often *crucial* for the success of data analysis! Input data frequently comes with all kinds of issues and failing to clean them up will lead to error messages when running analysis algorithms (in the best case) or to biased/erroneous results that go unnoticed (in the worst case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports <a id=imports></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Images\n",
    "from tifffile import imread, imsave\n",
    "\n",
    "# Statistics & machine learning\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Networks\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Interactivity\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data <a id=loading></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data \n",
    "datafile_path = 'data/BBBC018_v1_features.pkl'\n",
    "\n",
    "# Load dataframe\n",
    "df = pd.read_pickle(datafile_path)\n",
    "\n",
    "# Report\n",
    "print( df.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Checks for Common Problems <a id=checks></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Are there any columns (except imageID) that do not have numerical data?\n",
    "\n",
    "# Check\n",
    "print( df.select_dtypes(exclude=[np.number]).columns )  # ->> No, it's only imageID!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Are there any duplicated columns or rows?\n",
    "\n",
    "# Check rows\n",
    "print ( df.duplicated().nonzero() )    # ->> No, looks fine!\n",
    "\n",
    "# Check columns\n",
    "print ( df.T.duplicated().nonzero() )  # ->> Yes, there are! Remove them and check again!\n",
    "\n",
    "# Remove duplicate columns and check again\n",
    "df = df.drop(df.columns[df.T.duplicated()], axis=1)\n",
    "print ( df.T.duplicated().nonzero() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Are there any columns or rows that have NaN entries?\n",
    "\n",
    "# Find NaN columns\n",
    "print( df.loc[:, df.isnull().sum() > 0].columns )  # ->> There is one column with NaNs!\n",
    "\n",
    "# Find NaN rows\n",
    "print( df.isnull().any(axis=1).nonzero() )         # ->> There are many rows with NaNs!\n",
    "\n",
    "# Since all rows' NaNs are in one column, the easiest is to remove that column!\n",
    "df = df.dropna(axis=1)\n",
    "print( df.loc[:, df.isnull().sum() > 0].columns )\n",
    "print( df.isnull().any(axis=1).nonzero() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Are there any columns where all values are identical?\n",
    "# This can be checked looking for columns that have a standard deviation of zero.\n",
    "\n",
    "# Check\n",
    "print ( df.select_dtypes([np.number]).loc[:, df.select_dtypes([np.number]).std()==0].columns )  # ->> No, looks fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Visualization <a id=dataviz></a>\n",
    "\n",
    "As a first step, we need to get an idea of what our data \"looks like\". Things like `df.describe` are a starting point for that but they don't get us very far; we need plots! Lots and lots of plots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Boxplot <a id=bplot></a>\n",
    "\n",
    "A good starting point for looking at any kind of data that can be divided into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple boxplot\n",
    "\n",
    "# Prep\n",
    "fig = plt.figure(figsize=(18,5))\n",
    "\n",
    "# Create boxplot\n",
    "# Pandas dataframes come with a boxplot function. This is useful since it\n",
    "# provides some additional functionalities over matplotlib's standard boxplots,\n",
    "# as we will see later in the tutorial.\n",
    "df.boxplot()\n",
    "\n",
    "# Some formatting\n",
    "plt.grid(False)\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# Done\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Scatterplot <a id=iscatter></a>\n",
    "\n",
    "In multi-dimensional datasets such as this one, the limitations of plotting to the 2D or 3D space present a real problem. Fortunately, interactive plotting can to some extent solve this problem, as illustrated in this interactive scatterplot.\n",
    "\n",
    "<font color=green>**Exercise:**</font> Color the dots based on a third feature, which should be selectable from a third drop-down menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive scatterplot\n",
    "\n",
    "# Set interactivity\n",
    "@interact(x = list(df.columns),\n",
    "          y = list(df.columns))\n",
    "def make_interactive_scatterplot(x=df.columns[0], \n",
    "                                 y=df.columns[1]):\n",
    "\n",
    "    # Handle potential problems\n",
    "    if 'imageID' in [x,y]:\n",
    "        print(\"'imageID' is an invalid selection for this plot.\")\n",
    "        return\n",
    "    \n",
    "    # Prep\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    # Create scatterplot\n",
    "    plt.scatter(df[x], df[y], s=20,\n",
    "                edgecolor='k', alpha=0.5)\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    \n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Backmapping <a id=ibackmap></a>\n",
    "\n",
    "Since our data originally derives from images, one of the most interesting ways of visualizing it is to map it back onto the image as a colored overlay. This was already shown in the image analysis tutorial but here it is extended to allow interactive choice of various aspects of the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Backmapping onto images\n",
    "\n",
    "# Location of images & segmentations\n",
    "img_path = r'data\\BBBC018_v1_images_tif'\n",
    "seg_path = r'data\\BBBC018_v1_images_seg'\n",
    "\n",
    "# Set interactivity\n",
    "@interact(img_id  = list(set(df['imageID'])),\n",
    "          channel = ['DNA', 'pH3', 'actin'],\n",
    "          segtype = ['nucseg', 'cytseg'],\n",
    "          feature = list(df.columns),\n",
    "          alpha   = (0.0, 1.0, 0.1))\n",
    "def make_interactive_scatterplot(img_id  = list(set(df['imageID']))[0], \n",
    "                                 channel = 'actin',\n",
    "                                 segtype = 'cytseg',\n",
    "                                 feature = 'cyt-area-act',\n",
    "                                 alpha   = 0.4):\n",
    "\n",
    "    # Handle potential problems\n",
    "    if feature=='imageID':\n",
    "        print(\"'imageID' is an invalid feature for this plot.\")\n",
    "        return\n",
    "    \n",
    "    # Load image & segmentation\n",
    "    img = imread(os.path.join(img_path, img_id+'-'+channel+'_8bit.tif'))\n",
    "    seg = imread(os.path.join(seg_path, img_id+'-'+segtype+'.tif'))\n",
    "    \n",
    "    # Get feature values and standardize to 8bit\n",
    "    feat = np.array( df[df['imageID']==img_id][feature] )\n",
    "    feat = (feat - feat.min()) / (feat.max() - feat.min()) * 255.0\n",
    "    feat = feat.astype(np.uint8)\n",
    "    \n",
    "    # Recolor segmentation\n",
    "    seg_colored = np.zeros_like(seg).astype(np.uint8)\n",
    "    for cell_idx, cell_value in zip(np.unique(seg)[1:], feat):\n",
    "        seg_colored[seg==cell_idx] = cell_value\n",
    "    \n",
    "    # Prep\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    \n",
    "    # Display image\n",
    "    plt.imshow(img, interpolation='none', cmap='gray')\n",
    "    \n",
    "    # Overlay values\n",
    "    plt.imshow(np.ma.array(seg_colored, mask=seg_colored==0), \n",
    "               interpolation='none', cmap='viridis', alpha=alpha)\n",
    "    \n",
    "    # Add a title\n",
    "    plt.title('img: '+img_id+' | ch: '+channel+' | seg: '+segtype[:3]+' | feat: '+feature,\n",
    "              fontsize=18)\n",
    "    \n",
    "    # Other formatting\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Dimensional Analysis <a id=MDA></a>\n",
    "\n",
    "Whilst simple plots and summary statistics allow the investigation of individual measures and their relationships, the true power of large multi-dimensional datasets lies in the combined use of all the extracted features.\n",
    "\n",
    "Multi-dimensional data analysis closely intersects with the *machine learning* field. Therefore, two types of multi-dimensional analysis can be distinguished:\n",
    "\n",
    "- **Unsupervised methods** investigate the structure of the dataset to find patterns, such as clusters of similar cells.\n",
    "    - Here, we will...\n",
    "        - ...visualize the diversity of the cells in the \"phenotype space\" using PCA and tSNE\n",
    "        - ...cluster the cells into phenotypically similar groups using k-means clustering\n",
    "        - ...visualize cluster relationships and properties using a minimum spanning tree\n",
    "\n",
    "\n",
    "- **Supervised methods** relate the data to some pre-determined external piece of information, for example the classification of specific cell types based on pre-annotated training data. \n",
    "    - Here, we will...\n",
    "        - ...classify cells into mitotic and non-mitotic based on their phenotype, using the pH3 marker to create the pre-annotated training data\n",
    "        - ...analyze the differences between mitotic and non-mitotic cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Standardization <a id=featstand></a>\n",
    "\n",
    "Before doing any analysis, the different features/dimensions of the data need to be normalized such that they all can equally contribute to the analysis. Without normalization, the area of a cell might contribute more than the circumference, simply because the numbers measuring area are generally larger than those measuring circumferences - not because the area necessarily encodes more information.\n",
    "\n",
    "The most common normalization is called `normalization to zero mean and unit variance`, also known simply as `standardization` or `standard scaling` ([wiki](https://en.wikipedia.org/wiki/Feature_scaling#Standardization)). For each dimension, the mean is subtracted and the result is divided by the standard deviation, which makes the 'unit' of the axes into 'unit variance' and therefore encodes the relative differences of cells more than the absolute magnitude of values.\n",
    "\n",
    "<font color=green>**Exercise:**</font> In what situations might standardization be problematic? Can you think of (and implement) alternatives that might work better in such situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numerical columns (here only imageID)\n",
    "data_df = df.select_dtypes([np.number])\n",
    "\n",
    "# Show boxplot before standardization\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "data_df.boxplot(grid=False)\n",
    "fig.autofmt_xdate()\n",
    "plt.show()\n",
    "\n",
    "# Standardize to zero mean and unit variance\n",
    "scaled  = StandardScaler().fit_transform(data_df)\n",
    "data_df = pd.DataFrame(scaled, index=data_df.index, columns=data_df.columns)\n",
    "\n",
    "# Show boxplot after standardization\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "data_df.boxplot(grid=False)\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction by PCA <a id=pca></a>\n",
    "\n",
    "... ([wiki](https://en.wikipedia.org/wiki/Principal_component_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(data_df)\n",
    "pca_df = pd.DataFrame(pca.transform(data_df), \n",
    "                      index=data_df.index, \n",
    "                      columns=['PC'+str(i) for i in range(1,data_df.shape[1]+1)])\n",
    "\n",
    "# Look at explained variance ratio\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('PCs'); plt.ylabel('expl_var_ratio')\n",
    "plt.show()\n",
    "\n",
    "# Truncate to remove unimportant PCs\n",
    "pca_df = pca_df.iloc[:, :15]\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot PCs in interactive scatterplot\n",
    "\n",
    "# Set interactivity\n",
    "@interact(x = list(pca_df.columns),\n",
    "          y = list(pca_df.columns),\n",
    "          color = list(data_df.columns))\n",
    "def make_interactive_scatterplot(x=pca_df.columns[0], \n",
    "                                 y=pca_df.columns[1],\n",
    "                                 color=data_df.columns[0]):\n",
    "    \n",
    "    # Prep\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    \n",
    "    # Create scatterplot\n",
    "    plt.scatter(pca_df[x], pca_df[y], s=20,\n",
    "                c=data_df[color], alpha=0.5)\n",
    "    \n",
    "    # Labels\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    \n",
    "    # Limits\n",
    "    plt.xlim([np.percentile(pca_df[x], 0.5), np.percentile(pca_df[x], 99.5)])\n",
    "    plt.ylim([np.percentile(pca_df[x], 0.5), np.percentile(pca_df[x], 99.5)])\n",
    "    \n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction by tSNE <a id=tsne></a>\n",
    "\n",
    "... ([wiki](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tSNE\n",
    "\n",
    "# Random subsampling of cells\n",
    "sample = np.random.choice(np.arange(data_df.shape[0]), 2000, replace=False)\n",
    "\n",
    "# Perform tSNE\n",
    "# WARNING: The metaparameters (in particular perplexity) matter a lot for tSNE!\n",
    "#          See https://distill.pub/2016/misread-tsne/ for more information!\n",
    "tsne = TSNE(n_components=2, perplexity=30.0, learning_rate=200.0, n_iter=2000) \n",
    "tsne_df = pd.DataFrame(tsne.fit_transform(pca_df.iloc[sample, :]), \n",
    "                       index=data_df.iloc[sample,:].index, \n",
    "                       columns=['tSNE1', 'tSNE2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot tSNEs in interactive scatterplot\n",
    "\n",
    "# Set interactivity\n",
    "@interact(color = list(data_df.columns))\n",
    "def make_interactive_scatterplot(color=data_df.columns[0]):\n",
    "\n",
    "    # Prep\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "    # Create scatterplot\n",
    "    plt.scatter(tsne_df['tSNE1'], tsne_df['tSNE2'], s=20,\n",
    "                c=data_df.iloc[sample,:][color], alpha=0.5)\n",
    "\n",
    "    # Labels\n",
    "    plt.xlabel('tSNE1')\n",
    "    plt.ylabel('tSNE2')\n",
    "\n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with k-Means<a id=cluster></a>\n",
    "\n",
    "... ([wiki](https://en.wikipedia.org/wiki/K-means_clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple k-means\n",
    "\n",
    "# Perform clustering and get cluster labels\n",
    "kmeans = KMeans(n_clusters=12, n_jobs=2)\n",
    "kmeans.fit(pca_df)\n",
    "\n",
    "# Get labels and add to df\n",
    "labels = kmeans.labels_\n",
    "df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Exercise:**</font> There are many unsupervised clustering algorithms available in scikit-learn and all of them are easy to use in the same way as KMeans. Find and implement another one and think about ways of comparing the results of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot tSNE with KMeans labels colored\n",
    "\n",
    "# Prep\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "# Create scatterplot\n",
    "plt.scatter(tsne_df['tSNE1'], tsne_df['tSNE2'], s=20,\n",
    "            c=labels[sample], edgecolor='face', cmap='Set1')\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('tSNE1')\n",
    "plt.ylabel('tSNE2')\n",
    "\n",
    "# Done\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive boxplot grouped by cluster\n",
    "\n",
    "# Set interactivity\n",
    "@interact(feature=list(data_df.columns))\n",
    "def make_interactive_box(feature=data_df.columns[0]):\n",
    "    \n",
    "    # Create boxplot\n",
    "    df.boxplot(by='cluster', column=feature, grid=False, figsize=(12,6))\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel('Cluster', fontsize=18)\n",
    "    plt.ylabel(feature, fontsize=18)\n",
    "    plt.suptitle('')\n",
    "    plt.title('')\n",
    "    \n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Visualization by Minimum Spanning Tree <a id=mst></a>\n",
    "\n",
    "... ([wiki](https://en.wikipedia.org/wiki/Minimum_spanning_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create graph based on pairwise distance between cluster centers\n",
    "\n",
    "# Adjacency matrix\n",
    "dists = squareform(pdist(kmeans.cluster_centers_))\n",
    "\n",
    "# Graph from adjacency matrix\n",
    "G = nx.from_numpy_matrix(dists)\n",
    "\n",
    "# Minimum Spanning Tree\n",
    "T = nx.minimum_spanning_tree(G)\n",
    "\n",
    "# Show\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "nx.draw(G, ax=ax[0])\n",
    "nx.draw(T, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interactive display of minimal spanning tree of clusters\n",
    "\n",
    "# Get positions\n",
    "pos = nx.fruchterman_reingold_layout(T, random_state=46)\n",
    "\n",
    "# Get mean data per cluster\n",
    "cluster_df = df.groupby('cluster').mean()\n",
    "\n",
    "# Set interactivity\n",
    "@interact(feature=list(data_df.columns))\n",
    "def make_interactive_MST(feature=data_df.columns[0]):\n",
    "    \n",
    "    # Prep\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    ## Draw network\n",
    "    #nx.draw(T, pos=pos, width=2\n",
    "    #        node_color = cluster_df.iloc[np.array(T.nodes)][feature],\n",
    "    #        node_size  = df.groupby('cluster').count().iloc[:, 0],\n",
    "    #        edge_color = [e[-1]['weight'] for e in T.edges(data=True)])\n",
    "    \n",
    "    # Draw edges\n",
    "    p_edges = nx.draw_networkx_edges(T, pos=pos, width=3, edge_color='gray')\n",
    "    \n",
    "    # Draw nodes\n",
    "    nodes = nx.draw_networkx_nodes(T, pos=pos, node_size=500,\n",
    "                                   node_color=cluster_df.iloc[np.array(T.nodes)][feature])\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(nodes)\n",
    "    cbar.set_label(feature, labelpad=10, fontsize=18)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Mitotic Cells <a id=mitotic></a>\n",
    "\n",
    "... SVM ([wiki](https://en.wikipedia.org/wiki/Support_vector_machine)) ([sklearn](http://scikit-learn.org/stable/modules/svm.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use pH3 signal to create ground truth labels (True: \"in mitosis\" | False: \"not in mitosis\") \n",
    "\n",
    "# Check pH3 signal distribution with histogram\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.hist(df['nuc-mean_intensity-pH3'], bins=50)\n",
    "plt.xticks(range(0,130,5))\n",
    "plt.ylim([0, 500])\n",
    "plt.show()\n",
    "\n",
    "# Create ground truth\n",
    "ground_truth = (df['nuc-mean_intensity-pH3'] > 20).values\n",
    "print( ground_truth )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split into training and test set\n",
    "\n",
    "out = train_test_split(pca_df, ground_truth, test_size=0.3, random_state=43, stratify=ground_truth)\n",
    "X_train, X_test, y_train, y_test = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Support Vector Classification\n",
    "\n",
    "# Train linear SVC on training data\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check how well it worked\n",
    "\n",
    "# Compute accuracy: (TP+TN)/ALL\n",
    "accuracy = np.sum(y_pred==y_test) / y_pred.size\n",
    "print( \"Accuracy: \", accuracy )\n",
    "\n",
    "# Compute precision TP/ALL_T\n",
    "precision = np.sum( (y_pred==1) & (y_pred==y_test) ) / np.sum(y_test)\n",
    "print( \"Precision:\", precision )\n",
    "\n",
    "# Confusion matrix\n",
    "cmat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Show\n",
    "plt.imshow(cmat, interpolation='none', cmap='Blues')\n",
    "for (i, j), z in np.ndenumerate(cmat):\n",
    "    plt.text(j, i, z, ha='center', va='center')\n",
    "plt.xticks([0,1], [\"Non-Mitotic\", \"Mitotic\"])\n",
    "plt.yticks([0,1], [\"Non-Mitotic\", \"Mitotic\"], rotation=90)\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.ylabel(\"ground truth\")\n",
    "plt.show()\n",
    "\n",
    "# Note: This already works very well with just a linear SVC. In practice, a non-linear\n",
    "#       SVC (with a so-called 'RBF' kernel) is often better suited, which will require\n",
    "#       hyper-parameter optimization to yield the best possible results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cross validation\n",
    "\n",
    "# Run cross-validation\n",
    "cross_val = cross_validate(svc, pca_df, ground_truth, cv=5, scoring=['accuracy', 'precision'])\n",
    "\n",
    "# Print results\n",
    "print( cross_val['test_accuracy'] )\n",
    "print( cross_val['test_precision'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Highlighting in tSNE plot\n",
    "\n",
    "# Prep\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "# Create scatterplot\n",
    "plt.scatter(tsne_df['tSNE1'], tsne_df['tSNE2'], s=20,\n",
    "            c=svc.predict(pca_df)[sample], edgecolor='face', cmap='Set1_r')\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('tSNE1')\n",
    "plt.ylabel('tSNE2')\n",
    "\n",
    "# Done\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped Analysis and Hypothesis Testing <a id=grouped_and_hypot></a>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add mitotic label to df\n",
    "\n",
    "# Predict for everyone\n",
    "mitotic = svc.predict(pca_df)\n",
    "\n",
    "# Add to df\n",
    "df['mitotic'] = mitotic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grouped interactive boxplot\n",
    "\n",
    "# Set interactivity\n",
    "@interact(feature=list(data_df.columns))\n",
    "def make_interactive_box(feature=data_df.columns[0]):\n",
    "    \n",
    "    # Create boxplot\n",
    "    df.boxplot(by='mitotic', column=feature, grid=False, figsize=(4,6), fontsize=16, widths=0.6)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel('mitotic', fontsize=18)\n",
    "    plt.ylabel(feature, fontsize=18)\n",
    "    plt.suptitle('')\n",
    "    plt.title('')\n",
    "    \n",
    "    # Done\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple hypothesis tests\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Check if solidity is greater in mitotic cells\n",
    "s,p = mannwhitneyu(df.loc[ df['mitotic']]['cyt-solidity-act'],\n",
    "                   df.loc[~df['mitotic']]['cyt-solidity-act'],\n",
    "                   alternative='greater')\n",
    "print( 'MWU p-value:', p )\n",
    "\n",
    "# Check if area is greater in mitotic cells\n",
    "s,p = mannwhitneyu(df.loc[ df['mitotic']]['cyt-area-act'],\n",
    "                   df.loc[~df['mitotic']]['cyt-area-act'],\n",
    "                   alternative='greater')\n",
    "print( 'MWU p-value:', p )\n",
    "\n",
    "# WARNING: Large sample numbers tend to yield 'significant' p-values even for very small\n",
    "#          (and possibly only technical) differences. Be very careful in interpreting \n",
    "#          these measures and ask your resident statistician for complementary approaches\n",
    "#          to validate your results (e.g. effect size measures such as Cohen's d, or \n",
    "#          sampling-based methods such as bootstrapping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>**Exercise:**</font> There are many hypothesis tests available in `scipy.stats`. See if you can do a t-test instead of Mann-Whitney U for the data above (but don't forget that you first have to check if the data fits the assumptions of a t-test!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3base]",
   "language": "python",
   "name": "conda-env-py3base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
